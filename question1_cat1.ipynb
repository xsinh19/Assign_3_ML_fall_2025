{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Downloading Sherlock Holmes corpus...\n",
      "Warning: Could not find start/end markers. Using full text.\n",
      "Corpus preprocessing complete.\n",
      "Total tokens in corpus: 100339\n",
      "Final vocabulary size: 10000\n",
      "\n",
      "--- Vocabulary Report ---\n",
      "Vocabulary Size: 10000\n",
      "10 Most Frequent Words: [('.', 0), ('the', 4899), ('and', 2679), ('i', 2666), ('of', 2464), ('to', 2440), ('a', 2369), ('in', 1621), ('that', 1530), ('it', 1412), ('you', 1299)]\n",
      "10 Least Frequent Words (in vocab): [('grating.', 1), ('facetowards', 1), ('coarsely', 1), ('acoloured', 1), ('grime', 1), ('whichcovered', 1), ('ugliness.', 1), ('broadwheal', 1), ('itscontraction', 1), ('threeteeth', 1)]\n",
      "\n",
      "Saved 'vocab.pkl'. This is needed for the Streamlit app.\n",
      "Train tokens: 90305, Validation tokens: 10034\n",
      "\n",
      "Starting training for 16 model combinations...\n",
      "\n",
      "--- CONTEXT_LENGTH: 6 ---\n",
      "Training Model 1/16: model_ctx6_embed64_hidden1_act_relu\n",
      "  Epoch 5/200, Train Loss: 1.5684, Val Loss: 9.4968, Val Acc: 8.36%\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# === Part 1: Colab Training Script (PyTorch) ===\n",
    "#\n",
    "# This notebook will train all 32 model variations\n",
    "# and save them as.pth files for the Streamlit app.\n",
    "#\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "import re\n",
    "from collections import Counter\n",
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "# For analysis\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "#\n",
    "# === 1.1: Data Acquisition and Preprocessing ===\n",
    "#\n",
    "\n",
    "print(\"Downloading Sherlock Holmes corpus...\")\n",
    "url_sherlock = \"https://www.gutenberg.org/files/1661/1661-0.txt\"\n",
    "r = requests.get(url_sherlock)\n",
    "with open('sherlock_holmes.txt', 'wb') as f:\n",
    "    f.write(r.content)\n",
    "\n",
    "def preprocess_sherlock(filepath):\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Delimit the corpus\n",
    "    start_marker = \"THE ADVENTURES OF SHERLOCK HOLMES\"\n",
    "    end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "    try:\n",
    "        start_index = text.index(start_marker)\n",
    "        end_index = text.index(end_marker)\n",
    "        text = text[start_index:end_index]\n",
    "    except ValueError:\n",
    "        print(\"Warning: Could not find start/end markers. Using full text.\")\n",
    "\n",
    "    # Standard cleaning\n",
    "    text = text.lower()\n",
    "    text = text.replace('.', '. ') # Isolate the full stop\n",
    "    text = re.sub(r'[^a-z0-9 \\.]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "sherlock_text = preprocess_sherlock('sherlock_holmes.txt')\n",
    "print(\"Corpus preprocessing complete.\")\n",
    "\n",
    "\n",
    "#\n",
    "# === 1.2: Vocabulary Construction ===\n",
    "#\n",
    "\n",
    "# Tokenize\n",
    "tokens_sherlock = sherlock_text.split(' ')\n",
    "print(f\"Total tokens in corpus: {len(tokens_sherlock)}\")\n",
    "\n",
    "# Build vocabulary\n",
    "VOCAB_SIZE = 10000\n",
    "word_counts = Counter(tokens_sherlock)\n",
    "\n",
    "# Create vocabulary: top (VOCAB_SIZE - 2) words + <UNK> and <PAD>\n",
    "# We will use the '.' token as our padding token for context.\n",
    "vocabulary = [word for word, count in word_counts.most_common(VOCAB_SIZE - 2)]\n",
    "vocabulary.insert(0, '<UNK>') # Add <UNK> at index 0\n",
    "vocabulary.insert(1, '.')     # Add '.' at index 1\n",
    "\n",
    "# Create word-to-integer and integer-to-word mappings\n",
    "word_to_int = {word: i for i, word in enumerate(vocabulary)}\n",
    "int_to_word = {i: word for i, word in enumerate(vocabulary)}\n",
    "\n",
    "# Map all tokens in the corpus to integers\n",
    "tokens_int = []\n",
    "for word in tokens_sherlock:\n",
    "    tokens_int.append(word_to_int.get(word, word_to_int['<UNK>']))\n",
    "\n",
    "print(f\"Final vocabulary size: {len(vocabulary)}\")\n",
    "\n",
    "# --- Report: Vocabulary Analysis ---\n",
    "print(\"\\n--- Vocabulary Report ---\")\n",
    "print(f\"Vocabulary Size: {len(vocabulary)}\")\n",
    "\n",
    "# 10 most frequent (excluding <UNK>)\n",
    "most_freq = [(int_to_word[i], word_counts[int_to_word[i]]) for i in range(1, 12)]\n",
    "print(f\"10 Most Frequent Words: {most_freq}\")\n",
    "\n",
    "# 10 least frequent in our vocab\n",
    "least_freq = [(word, word_counts[word]) for word in vocabulary[-10:]]\n",
    "print(f\"10 Least Frequent Words (in vocab): {least_freq}\")\n",
    "\n",
    "# Save the vocab artifacts for the Streamlit app\n",
    "# This is the most important file besides the models\n",
    "vocab_artifacts = {\n",
    "    'word_to_int': word_to_int,\n",
    "    'int_to_word': int_to_word,\n",
    "    'vocab_size': len(vocabulary)\n",
    "}\n",
    "with open('vocab.pkl', 'wb') as f:\n",
    "    pickle.dump(vocab_artifacts, f)\n",
    "print(\"\\nSaved 'vocab.pkl'. This is needed for the Streamlit app.\")\n",
    "\n",
    "\n",
    "#\n",
    "# === 1.3: PyTorch Custom Dataset ===\n",
    "#\n",
    "\n",
    "class SlidingWindowDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for creating sliding window (context, target) pairs.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens, context_length):\n",
    "        self.tokens = tokens\n",
    "        self.context_length = context_length\n",
    "\n",
    "        # Pad the beginning of the token list to create context for the first words\n",
    "        # We use the integer for '.' as our padding token\n",
    "        pad_token_int = word_to_int['.']\n",
    "        self.padded_tokens = [pad_token_int] * self.context_length + self.tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        # Total number of (context, target) pairs\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The context starts from the padded list\n",
    "        context = self.padded_tokens[idx : idx + self.context_length]\n",
    "        # The target is the next word in the *original* token list\n",
    "        target = self.tokens[idx]\n",
    "\n",
    "        # Return as LongTensors\n",
    "        return torch.tensor(context, dtype=torch.long), torch.tensor(target, dtype=torch.long)\n",
    "\n",
    "# Split data: 90% train, 10% validation\n",
    "split_idx = int(len(tokens_int) * 0.9)\n",
    "train_tokens = tokens_int[:split_idx]\n",
    "val_tokens = tokens_int[split_idx:]\n",
    "print(f\"Train tokens: {len(train_tokens)}, Validation tokens: {len(val_tokens)}\")\n",
    "\n",
    "\n",
    "#\n",
    "# === 1.4: Model Design (PyTorch nn.Module) ===\n",
    "#\n",
    "class MLPTextGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP-based N-gram model as specified in the query.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, context_length, hidden_layers, activation):\n",
    "        super(MLPTextGenerator, self).__init__()\n",
    "\n",
    "        self.context_length = context_length\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_hidden = hidden_layers\n",
    "\n",
    "        # 1. Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # 2. Flatten Layer\n",
    "        self.flatten = nn.Flatten()\n",
    "\n",
    "        # 3. Activation Function\n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        # 4. Hidden Layers\n",
    "        input_size = context_length * embed_dim\n",
    "        self.hidden1 = nn.Linear(input_size, 1024)\n",
    "\n",
    "        if self.num_hidden == 2:\n",
    "            self.hidden2 = nn.Linear(1024, 1024)\n",
    "\n",
    "        # 5. Output Layer\n",
    "        self.output = nn.Linear(1024, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, context_length)\n",
    "        x = self.embedding(x)\n",
    "        # x shape: (batch_size, context_length, embed_dim)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        # x shape: (batch_size, context_length * embed_dim)\n",
    "\n",
    "        x = self.activation(self.hidden1(x))\n",
    "        # x shape: (batch_size, 1024)\n",
    "\n",
    "        if self.num_hidden == 2:\n",
    "            x = self.activation(self.hidden2(x))\n",
    "\n",
    "        # Logits are returned. nn.CrossEntropyLoss applies log_softmax internally.\n",
    "        logits = self.output(x)\n",
    "        # logits shape: (batch_size, vocab_size)\n",
    "        return logits\n",
    "\n",
    "\n",
    "#\n",
    "# === 1.5: Hyperparameter Training Loop ===\n",
    "#\n",
    "\n",
    "# Create a directory to store the models\n",
    "MODEL_DIR = \"trained_models\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "CONTEXT_LENGTHS = [6, 7, 8, 9]\n",
    "EMBED_DIMS = [64] # Corrected from scalar to list\n",
    "HIDDEN_LAYERS = [1, 2]\n",
    "ACTIVATIONS = ['relu', 'tanh']\n",
    "\n",
    "# Training settings\n",
    "EPOCHS = 200  # Reduced for demonstration. 50-100 is better.\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.001\n",
    "VOCAB_SIZE = len(vocabulary)\n",
    "\n",
    "total_models = len(CONTEXT_LENGTHS) * len(EMBED_DIMS) * len(HIDDEN_LAYERS) * len(ACTIVATIONS)\n",
    "print(f\"\\nStarting training for {total_models} model combinations...\")\n",
    "model_count = 0\n",
    "\n",
    "# Store history of the last model for plotting\n",
    "last_model_history = {}\n",
    "\n",
    "for context in CONTEXT_LENGTHS:\n",
    "    print(f\"\\n--- CONTEXT_LENGTH: {context} ---\")\n",
    "\n",
    "    # 1. Create Datasets and DataLoaders for this context size\n",
    "    train_dataset = SlidingWindowDataset(train_tokens, context)\n",
    "    val_dataset = SlidingWindowDataset(val_tokens, context)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "    for embed in EMBED_DIMS:\n",
    "        for hidden in HIDDEN_LAYERS:\n",
    "            for act in ACTIVATIONS:\n",
    "                model_count += 1\n",
    "                start_time = time.time()\n",
    "                model_name = f\"model_ctx{context}_embed{embed}_hidden{hidden}_act_{act}\"\n",
    "                model_filename = os.path.join(MODEL_DIR, f\"{model_name}.pth\")\n",
    "\n",
    "                print(f\"Training Model {model_count}/{total_models}: {model_name}\")\n",
    "\n",
    "                # 2. Instantiate Model\n",
    "                model = MLPTextGenerator(VOCAB_SIZE, embed, context, hidden, act).to(device)\n",
    "\n",
    "                # 3. Setup Optimizer and Loss\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "                best_val_loss = float('inf')\n",
    "                history = {'train_loss': [], 'val_loss': [], 'val_acc': []} # Corrected syntax\n",
    "\n",
    "                for epoch in range(EPOCHS):\n",
    "                    # --- Training ---\n",
    "                    model.train()\n",
    "                    train_loss_epoch = 0\n",
    "                    for inputs, targets in train_loader:\n",
    "                        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                        optimizer.zero_grad()\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, targets)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        train_loss_epoch += loss.item()\n",
    "\n",
    "                    avg_train_loss = train_loss_epoch / len(train_loader)\n",
    "                    history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "                    # --- Validation ---\n",
    "                    model.eval()\n",
    "                    val_loss_epoch = 0\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    with torch.no_grad():\n",
    "                        for inputs, targets in val_loader:\n",
    "                            inputs, targets = inputs.to(device), targets.to(device)\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, targets)\n",
    "                            val_loss_epoch += loss.item()\n",
    "\n",
    "                            _, predicted = torch.max(outputs.data, 1)\n",
    "                            total += targets.size(0)\n",
    "                            correct += (predicted == targets).sum().item()\n",
    "\n",
    "                    avg_val_loss = val_loss_epoch / len(val_loader)\n",
    "                    val_accuracy = 100 * correct / total\n",
    "                    history['val_loss'].append(avg_val_loss)\n",
    "                    history['val_acc'].append(val_accuracy)\n",
    "\n",
    "                    if (epoch + 1) % 5 == 0:\n",
    "                        print(f\"  Epoch {epoch+1}/{EPOCHS}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "                    # 4. Save the best model\n",
    "                    if avg_val_loss < best_val_loss:\n",
    "                        best_val_loss = avg_val_loss\n",
    "                        torch.save(model.state_dict(), model_filename) #\n",
    "\n",
    "                end_time = time.time()\n",
    "                print(f\"Finished training {model_name}. Time: {end_time - start_time:.2f}s. Model saved to {model_filename}\")\n",
    "                print(f\"  Final Validation Loss: {best_val_loss:.4f}\")\n",
    "                print(f\"  Final Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "                # Save history for the last model\n",
    "                if model_count == total_models:\n",
    "                    last_model_history = history\n",
    "                    last_model_trained = model\n",
    "\n",
    "print(\"\\nAll models trained and saved.\")\n",
    "\n",
    "\n",
    "#\n",
    "# === 1.6: Example Predictions (from last model) ===\n",
    "#\n",
    "def generate_text(model, seed_text, num_words_to_gen, context_length):\n",
    "    model.eval()\n",
    "    generated_text = seed_text.lower()\n",
    "    seed_tokens = generated_text.split()\n",
    "\n",
    "    pad_token_int = word_to_int['.']\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(num_words_to_gen):\n",
    "            # 1. Prepare input\n",
    "            context_tokens = seed_tokens[-context_length:]\n",
    "\n",
    "            # 2. Pad if seed is too short\n",
    "            if len(context_tokens) < context_length:\n",
    "                pad_list = ['.'] * (context_length - len(context_tokens))\n",
    "                context_tokens = pad_list + context_tokens\n",
    "\n",
    "            # 3. Convert to integers, handling OOV\n",
    "            context_ints = [] # Corrected syntax\n",
    "            for word in context_tokens:\n",
    "                context_ints.append(word_to_int.get(word, word_to_int['<UNK>']))\n",
    "\n",
    "            # 4. Predict\n",
    "            X_input = torch.tensor([context_ints], dtype=torch.long).to(device)\n",
    "            y_pred_logits = model(X_input)\n",
    "\n",
    "            # 5. Get greedy prediction (argmax)\n",
    "            y_pred_int = torch.argmax(y_pred_logits, dim=1).item()\n",
    "\n",
    "            # 6. Convert back to word\n",
    "            y_pred_word = int_to_word.get(y_pred_int, '<UNK>')\n",
    "\n",
    "            # 7. Append\n",
    "            generated_text += \" \" + y_pred_word\n",
    "            seed_tokens.append(y_pred_word)\n",
    "\n",
    "    return generated_text\n",
    "\n",
    "print(\"\\n--- Generation Examples (from last trained model) ---\")\n",
    "last_context = CONTEXT_LENGTHS[-1]\n",
    "print(f\"SEED: 'holmes was a man of'\\n{generate_text(last_model_trained, 'holmes was a man of', 50, last_context)}\\n\")\n",
    "print(f\"SEED: 'the crime was committed at'\\n{generate_text(last_model_trained, 'the crime was committed at', 50, last_context)}\\n\")\n",
    "\n",
    "#\n",
    "# === 1.7: Embedding Visualization (from last model) ===\n",
    "#\n",
    "print(\"--- Loss Plot (from last trained model) ---\")\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(last_model_history['train_loss'], label='Training Loss')\n",
    "plt.plot(last_model_history['val_loss'], label='Validation Loss')\n",
    "plt.title('Training vs. Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(last_model_history['val_acc'], label='Validation Accuracy')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- t-SNE Embedding Visualization (from last trained model) ---\")\n",
    "# 1. Extract embedding weights\n",
    "embedding_weights = last_model_trained.embedding.weight.data.cpu().numpy()\n",
    "print(f\"Embedding weights shape: {embedding_weights.shape}\") #\n",
    "\n",
    "# 2. Select specific words to visualize\n",
    "words_to_visualize = {\n",
    "    'names': ['holmes', 'watson', 'adler', 'moriarty', 'lestrade'],\n",
    "    'locations': ['london', 'baker', 'street', 'room', 'house', 'city'],\n",
    "    'speech_verbs': ['said', 'observed', 'remarked', 'answered', 'cried', 'replied'],\n",
    "    'evidence': ['pipe', 'cigar', 'dust', 'blood', 'footprint', 'clue'],\n",
    "    'pronouns': ['i', 'he', 'she', 'you', 'his', 'my', 'your']\n",
    "}\n",
    "\n",
    "word_vectors = [] # Corrected syntax\n",
    "labels = [] # Corrected syntax\n",
    "colors = [] # Corrected syntax\n",
    "color_map = plt.get_cmap('tab10')\n",
    "\n",
    "for i, (group_name, word_list) in enumerate(words_to_visualize.items()):\n",
    "    for word in word_list:\n",
    "        if word in word_to_int:\n",
    "            word_vectors.append(embedding_weights[word_to_int[word]])\n",
    "            labels.append(word)\n",
    "            colors.append(color_map(i))\n",
    "\n",
    "word_vectors = np.array(word_vectors)\n",
    "\n",
    "# 3. Apply t-SNE [7, 8, 9, 10]\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=10)\n",
    "tsne_results = tsne.fit_transform(word_vectors)\n",
    "\n",
    "# 4. Plot\n",
    "plt.figure(figsize=(16, 12))\n",
    "plt.scatter(tsne_results[:, 0], tsne_results[:, 1], c=colors)\n",
    "\n",
    "# Add annotations [11, 12]\n",
    "for i, label in enumerate(labels):\n",
    "    plt.annotate(label, (tsne_results[i, 0], tsne_results[i, 1]))\n",
    "\n",
    "plt.title('t-SNE Visualization of Sherlock Holmes Word Embeddings')\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "# === 1.8: Download Artifacts ===\n",
    "#\n",
    "# Finally, zip the required files for download\n",
    "print(\"Zipping artifacts...\")\n",
    "!zip -r sherlock_models_and_vocab.zip trained_models/ vocab.pkl\n",
    "\n",
    "print(\"\\n--- ACTION REQUIRED ---\")\n",
    "print(\"Training complete. Please download 'sherlock_models_and_vocab.zip' from the Colab file browser.\")\n",
    "print(\"You will also need the Streamlit 'app.py' script from Part 2.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
